{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] --fmri_path FMRI_PATH --labels_path\n",
      "                             LABELS_PATH [--num_epochs NUM_EPOCHS]\n",
      "                             [--initial_lr INITIAL_LR] [--patience PATIENCE]\n",
      "                             [--factor FACTOR] [--min_lr MIN_LR]\n",
      "                             [--hidden_layers HIDDEN_LAYERS [HIDDEN_LAYERS ...]]\n",
      "                             [--dropout DROPOUT] [--batch_size BATCH_SIZE]\n",
      "                             [--checkpoint_path CHECKPOINT_PATH]\n",
      "                             [--log_level {DEBUG,INFO,WARNING,ERROR,CRITICAL}]\n",
      "                             [--device {cpu,cuda}]\n",
      "ipykernel_launcher.py: error: ambiguous option: --f=/home/songlinzhao/.local/share/jupyter/runtime/kernel-v3e959ad9022331b61b784349e060baa32158abfd9.json could match --fmri_path, --factor\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import logging\n",
    "from typing import Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    confusion_matrix,\n",
    "    roc_auc_score,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def setup_logging(log_level: str = \"INFO\") -> None:\n",
    "    \"\"\"\n",
    "    Configure logging for the script.\n",
    "\n",
    "    Args:\n",
    "        log_level (str, optional): Logging level. Defaults to \"INFO\".\n",
    "    \"\"\"\n",
    "    numeric_level = getattr(logging, log_level.upper(), None)\n",
    "    if not isinstance(numeric_level, int):\n",
    "        raise ValueError(f\"Invalid log level: {log_level}\")\n",
    "\n",
    "    logging.basicConfig(\n",
    "        level=numeric_level,\n",
    "        format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "        handlers=[logging.StreamHandler()],\n",
    "    )\n",
    "\n",
    "\n",
    "def load_data(fmri_path: str, labels_path: str) -> Tuple:\n",
    "    \"\"\"\n",
    "    Load and preprocess fMRI and label data from CSV files.\n",
    "\n",
    "    Args:\n",
    "        fmri_path (str): Path to the fMRI data CSV file.\n",
    "        labels_path (str): Path to the labels CSV file.\n",
    "\n",
    "    Returns:\n",
    "        Tuple: (features as NumPy array, labels as NumPy array)\n",
    "    \"\"\"\n",
    "    logging.info(f\"Loading fMRI data from {fmri_path}\")\n",
    "    fmri_data = pd.read_csv(fmri_path)\n",
    "    logging.info(f\"Loading labels data from {labels_path}\")\n",
    "    labels_data = pd.read_csv(labels_path)\n",
    "\n",
    "    # Ensure 'IID' column exists\n",
    "    if 'IID' not in fmri_data.columns or 'IID' not in labels_data.columns:\n",
    "        raise ValueError(\"Both fMRI and labels data must contain an 'IID' column.\")\n",
    "\n",
    "    # Set IID as index for both datasets\n",
    "    fmri_data.set_index(\"IID\", inplace=True)\n",
    "    labels_data.set_index(\"IID\", inplace=True)\n",
    "\n",
    "    # Filter the fMRI data to include only IIDs present in the labels data\n",
    "    filtered_fmri_data = fmri_data.loc[labels_data.index]\n",
    "    logging.info(f\"Filtered fMRI data shape: {filtered_fmri_data.shape}\")\n",
    "\n",
    "    # Use DIA as the label\n",
    "    if \"DIA\" not in labels_data.columns:\n",
    "        raise ValueError(\"Labels data must contain a 'DIA' column.\")\n",
    "    labels = labels_data[\"DIA\"]\n",
    "\n",
    "    return filtered_fmri_data.values, labels.values\n",
    "\n",
    "\n",
    "def prepare_tensors(\n",
    "    X: torch.Tensor,\n",
    "    y: torch.Tensor,\n",
    "    test_size: float = 0.2,\n",
    "    val_size: float = 0.1,\n",
    "    random_state: int = 42,\n",
    "    device: torch.device = torch.device(\"cpu\"),\n",
    ") -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Split data into training, validation, and testing sets and convert them to PyTorch tensors.\n",
    "\n",
    "    Args:\n",
    "        X (torch.Tensor): Feature matrix.\n",
    "        y (torch.Tensor): Labels.\n",
    "        test_size (float, optional): Proportion of the dataset to include in the test split. Defaults to 0.2.\n",
    "        val_size (float, optional): Proportion of the training set to include in the validation split. Defaults to 0.1.\n",
    "        random_state (int, optional): Seed used by the random number generator. Defaults to 42.\n",
    "        device (torch.device, optional): Device to which tensors are moved. Defaults to CPU.\n",
    "\n",
    "    Returns:\n",
    "        Tuple: (X_train, X_val, X_test, y_train, y_val, y_test)\n",
    "    \"\"\"\n",
    "    logging.info(\"Splitting data into training and testing sets\")\n",
    "    X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state, stratify=y\n",
    "    )\n",
    "\n",
    "    logging.info(\"Splitting training data into training and validation sets\")\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train_full, y_train_full, test_size=val_size, random_state=random_state, stratify=y_train_full\n",
    "    )\n",
    "\n",
    "    # Convert to tensors and move to device\n",
    "    logging.info(\"Converting data to PyTorch tensors\")\n",
    "    X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "    X_val = torch.tensor(X_val, dtype=torch.float32).to(device)\n",
    "    X_test = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "    y_train = torch.tensor(y_train, dtype=torch.long).to(device)\n",
    "    y_val = torch.tensor(y_val, dtype=torch.long).to(device)\n",
    "    y_test = torch.tensor(y_test, dtype=torch.long).to(device)\n",
    "\n",
    "    logging.info(f\"Training set size: {X_train.shape[0]}\")\n",
    "    logging.info(f\"Validation set size: {X_val.shape[0]}\")\n",
    "    logging.info(f\"Testing set size: {X_test.shape[0]}\")\n",
    "\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \"\"\"Multi-Layer Perceptron (MLP) model.\"\"\"\n",
    "\n",
    "    def __init__(self, input_size: int, hidden_layers: list, dropout: float = 0.2):\n",
    "        \"\"\"\n",
    "        Initialize the MLP model.\n",
    "\n",
    "        Args:\n",
    "            input_size (int): Number of input features.\n",
    "            hidden_layers (list): List of integers specifying the number of neurons in each hidden layer.\n",
    "            dropout (float, optional): Dropout rate. Defaults to 0.2.\n",
    "        \"\"\"\n",
    "        super(MLP, self).__init__()\n",
    "        layers = []\n",
    "        previous_size = input_size\n",
    "\n",
    "        for idx, layer_size in enumerate(hidden_layers):\n",
    "            layers.append(nn.Linear(previous_size, layer_size))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "            previous_size = layer_size\n",
    "            logging.debug(f\"Added layer {idx+1}: Linear({previous_size} -> {layer_size})\")\n",
    "\n",
    "        layers.append(nn.Linear(previous_size, 2))  # 2 output classes\n",
    "        self.network = nn.Sequential(*layers)\n",
    "        logging.info(f\"MLP architecture: {self.network}\")\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.network(x)\n",
    "\n",
    "\n",
    "def train(\n",
    "    model: nn.Module,\n",
    "    criterion: nn.Module,\n",
    "    optimizer: optim.Optimizer,\n",
    "    scheduler: ReduceLROnPlateau,\n",
    "    X_train: torch.Tensor,\n",
    "    y_train: torch.Tensor,\n",
    "    X_val: torch.Tensor,\n",
    "    y_val: torch.Tensor,\n",
    "    num_epochs: int = 10000,\n",
    "    device: torch.device = torch.device(\"cpu\"),\n",
    "    checkpoint_path: str = \"best_model.pth\",\n",
    "    log_interval: int = 1000,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Train the MLP model.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The MLP model.\n",
    "        criterion (nn.Module): Loss function.\n",
    "        optimizer (optim.Optimizer): Optimizer.\n",
    "        scheduler (ReduceLROnPlateau): Learning rate scheduler.\n",
    "        X_train (torch.Tensor): Training features.\n",
    "        y_train (torch.Tensor): Training labels.\n",
    "        X_val (torch.Tensor): Validation features.\n",
    "        y_val (torch.Tensor): Validation labels.\n",
    "        num_epochs (int, optional): Number of training epochs. Defaults to 10000.\n",
    "        device (torch.device, optional): Device for computation. Defaults to CPU.\n",
    "        checkpoint_path (str, optional): Path to save the best model. Defaults to \"best_model.pth\".\n",
    "        log_interval (int, optional): Interval (in epochs) for logging. Defaults to 1000.\n",
    "    \"\"\"\n",
    "    best_loss = float(\"inf\")\n",
    "    logging.info(\"Starting training\")\n",
    "\n",
    "    for epoch in tqdm(range(1, num_epochs + 1), desc=\"Training\"):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(X_train)\n",
    "        loss = criterion(outputs, y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_outputs = model(X_val)\n",
    "            val_loss = criterion(val_outputs, y_val)\n",
    "\n",
    "        # Step the scheduler based on validation loss\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        # Save the best model\n",
    "        if val_loss.item() < best_loss:\n",
    "            best_loss = val_loss.item()\n",
    "            torch.save(model.state_dict(), checkpoint_path)\n",
    "            logging.debug(f\"Saved new best model with validation loss: {best_loss:.4f}\")\n",
    "\n",
    "        # Logging\n",
    "        if epoch % log_interval == 0 or epoch == 1:\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            logging.info(\n",
    "                f\"Epoch [{epoch}/{num_epochs}], \"\n",
    "                f\"Train Loss: {loss.item():.4f}, \"\n",
    "                f\"Val Loss: {val_loss.item():.4f}, \"\n",
    "                f\"LR: {current_lr:.6f}\"\n",
    "            )\n",
    "\n",
    "    logging.info(\"Training completed\")\n",
    "\n",
    "\n",
    "def evaluate(\n",
    "    model: nn.Module,\n",
    "    X_test: torch.Tensor,\n",
    "    y_test: torch.Tensor,\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Evaluate the trained model on the test set.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The trained MLP model.\n",
    "        X_test (torch.Tensor): Test features.\n",
    "        y_test (torch.Tensor): Test labels.\n",
    "\n",
    "    Returns:\n",
    "        dict: Evaluation metrics.\n",
    "    \"\"\"\n",
    "    logging.info(\"Evaluating the model on the test set\")\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(X_test)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "    # Convert predictions and labels to numpy arrays\n",
    "    y_pred = predicted.cpu().numpy()\n",
    "    y_true = y_test.cpu().numpy()\n",
    "\n",
    "    # Calculate metrics\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    sensitivity = recall_score(y_true, y_pred, pos_label=1)  # Recall for class 1\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "    # Calculate specificity (recall for class 0)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    specificity = tn / (tn + fp)\n",
    "\n",
    "    # AUROC (binary case)\n",
    "    auroc = roc_auc_score(y_true, predicted.cpu())\n",
    "\n",
    "    metrics = {\n",
    "        \"Accuracy\": acc,\n",
    "        \"Sensitivity (Recall for class 1)\": sensitivity,\n",
    "        \"Specificity (Recall for class 0)\": specificity,\n",
    "        \"F1-Score\": f1,\n",
    "        \"AUROC\": auroc,\n",
    "    }\n",
    "\n",
    "    logging.info(\"Evaluation Metrics:\")\n",
    "    for metric, value in metrics.items():\n",
    "        logging.info(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def parse_arguments() -> argparse.Namespace:\n",
    "    \"\"\"\n",
    "    Parse command-line arguments.\n",
    "\n",
    "    Returns:\n",
    "        argparse.Namespace: Parsed arguments.\n",
    "    \"\"\"\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"Train an MLP model for ADHD classification with auto learning rate adjustment.\"\n",
    "    )\n",
    "\n",
    "    # Data paths\n",
    "    parser.add_argument(\n",
    "        \"--fmri_path\",\n",
    "        type=str,\n",
    "        required=True,\n",
    "        help=\"Path to the fMRI data CSV file.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--labels_path\",\n",
    "        type=str,\n",
    "        required=True,\n",
    "        help=\"Path to the labels CSV file.\",\n",
    "    )\n",
    "\n",
    "    # Training parameters\n",
    "    parser.add_argument(\n",
    "        \"--num_epochs\",\n",
    "        type=int,\n",
    "        default=10000,\n",
    "        help=\"Number of training epochs.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--initial_lr\",\n",
    "        type=float,\n",
    "        default=0.0001,\n",
    "        help=\"Initial learning rate for the optimizer.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--patience\",\n",
    "        type=int,\n",
    "        default=10,\n",
    "        help=\"Number of epochs with no improvement after which learning rate will be reduced.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--factor\",\n",
    "        type=float,\n",
    "        default=0.5,\n",
    "        help=\"Factor by which the learning rate will be reduced.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--min_lr\",\n",
    "        type=float,\n",
    "        default=1e-6,\n",
    "        help=\"Minimum learning rate.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--hidden_layers\",\n",
    "        type=int,\n",
    "        nargs='+',\n",
    "        default=[512, 256, 128, 64, 32],\n",
    "        help=\"List of hidden layer sizes. Example: --hidden_layers 512 256 128\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--dropout\",\n",
    "        type=float,\n",
    "        default=0.2,\n",
    "        help=\"Dropout rate between layers.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--batch_size\",\n",
    "        type=int,\n",
    "        default=None,\n",
    "        help=\"Batch size for training. If not set, uses full batch.\",\n",
    "    )\n",
    "\n",
    "    # Checkpoint and logging\n",
    "    parser.add_argument(\n",
    "        \"--checkpoint_path\",\n",
    "        type=str,\n",
    "        default=\"best_model.pth\",\n",
    "        help=\"Path to save the best model checkpoint.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--log_level\",\n",
    "        type=str,\n",
    "        default=\"INFO\",\n",
    "        choices=[\"DEBUG\", \"INFO\", \"WARNING\", \"ERROR\", \"CRITICAL\"],\n",
    "        help=\"Logging level.\",\n",
    "    )\n",
    "\n",
    "    # Device configuration\n",
    "    parser.add_argument(\n",
    "        \"--device\",\n",
    "        type=str,\n",
    "        default=\"cpu\",\n",
    "        choices=[\"cpu\", \"cuda\"],\n",
    "        help=\"Device to run the training on ('cpu' or 'cuda').\",\n",
    "    )\n",
    "\n",
    "    return parser.parse_args()\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Parse command-line arguments\n",
    "    args = parse_arguments()\n",
    "\n",
    "    # Setup logging\n",
    "    setup_logging(args.log_level)\n",
    "\n",
    "    # Set device\n",
    "    if args.device == \"cuda\" and not torch.cuda.is_available():\n",
    "        logging.warning(\"CUDA is not available. Falling back to CPU.\")\n",
    "        device = torch.device(\"cpu\")\n",
    "    else:\n",
    "        device = torch.device(args.device)\n",
    "    logging.info(f\"Using device: {device}\")\n",
    "\n",
    "    # Validate data paths\n",
    "    if not os.path.isfile(args.fmri_path):\n",
    "        logging.error(f\"fMRI data file not found at {args.fmri_path}\")\n",
    "        raise FileNotFoundError(f\"fMRI data file not found at {args.fmri_path}\")\n",
    "    if not os.path.isfile(args.labels_path):\n",
    "        logging.error(f\"Labels data file not found at {args.labels_path}\")\n",
    "        raise FileNotFoundError(f\"Labels data file not found at {args.labels_path}\")\n",
    "\n",
    "    # Load and preprocess data\n",
    "    X, y = load_data(args.fmri_path, args.labels_path)\n",
    "\n",
    "    # Prepare tensors\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = prepare_tensors(\n",
    "        X, y, device=device\n",
    "    )\n",
    "\n",
    "    # Initialize the model\n",
    "    input_size = X_train.shape[1]\n",
    "    model = MLP(input_size, hidden_layers=args.hidden_layers, dropout=args.dropout).to(device)\n",
    "\n",
    "    # Define loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=args.initial_lr)\n",
    "\n",
    "    # Define learning rate scheduler\n",
    "    scheduler = ReduceLROnPlateau(\n",
    "        optimizer,\n",
    "        mode=\"min\",\n",
    "        factor=args.factor,\n",
    "        patience=args.patience,\n",
    "        verbose=True,\n",
    "        min_lr=args.min_lr,\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    train(\n",
    "        model=model,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler,\n",
    "        X_train=X_train,\n",
    "        y_train=y_train,\n",
    "        X_val=X_val,\n",
    "        y_val=y_val,\n",
    "        num_epochs=args.num_epochs,\n",
    "        device=device,\n",
    "        checkpoint_path=args.checkpoint_path,\n",
    "        log_interval=1000,  # Can also be made an argument if needed\n",
    "    )\n",
    "\n",
    "    # Load the best model\n",
    "    logging.info(\"Loading the best model for evaluation\")\n",
    "    model.load_state_dict(torch.load(args.checkpoint_path))\n",
    "\n",
    "    # Evaluate the model\n",
    "    metrics = evaluate(model, X_test, y_test)\n",
    "\n",
    "    # Optionally, save metrics to a file\n",
    "    metrics_path = os.path.splitext(args.checkpoint_path)[0] + \"_metrics.txt\"\n",
    "    with open(metrics_path, 'w') as f:\n",
    "        for metric, value in metrics.items():\n",
    "            f.write(f\"{metric}: {value:.4f}\\n\")\n",
    "    logging.info(f\"Saved evaluation metrics to {metrics_path}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "normodiff",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
